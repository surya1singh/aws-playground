{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0cc42e-85fb-4f08-8d3f-8f2708a19f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80530074-bd50-490f-9d8a-61f06b053acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "     \"--packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.1 pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b872570-0efc-4851-b3a9-1ec0ec502beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"msk-to-s3-alerts\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410f8ce1-9eda-4363-b961-a9b7b3f24eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP = \"kafka:9092\" #os.environ[\"KAFKA_BOOTSTRAP\"]          # \"b-1.xxx:9092,b-2.xxx:9092\"\n",
    "TOPIC = os.environ.get(\"KAFKA_TOPIC\", \"clickstream_events\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d638e7f-1e37-4701-8889-8cac34a5e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WATERMARK = os.environ.get(\"WATERMARK\", \"10 minutes\")\n",
    "ALERT_BAD_RATE = float(os.environ.get(\"ALERT_BAD_RATE\", \"0.02\"))  # 2%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "013319dc-4ec4-4c84-b950-554f391adc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_ts\", StringType(), False),  # parse to timestamp\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"device\", StructType([\n",
    "        StructField(\"os\", StringType(), True),\n",
    "        StructField(\"app_ver\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"ip\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7729d864-bf08-4830-a99d-381477f865e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    kdf = (\n",
    "        spark.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", BOOTSTRAP)\n",
    "        .option(\"subscribe\", TOPIC)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .load()\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(type(e), e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db7d4cc6-d939-4d8a-939d-50089117e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = kdf.select(\n",
    "    F.col(\"key\").cast(\"string\").alias(\"key\"),\n",
    "    F.col(\"value\").cast(\"string\").alias(\"value\"),\n",
    "    F.col(\"timestamp\").alias(\"kafka_ingest_ts\"),\n",
    "    F.col(\"topic\"),\n",
    "    F.col(\"partition\"),\n",
    "    F.col(\"offset\")\n",
    ")\n",
    "\n",
    "parsed = raw.withColumn(\"json\", F.from_json(F.col(\"value\"), event_schema))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249bd59b-5b40-4dd2-bced-7866c5d8de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "good = (\n",
    "    parsed\n",
    "    .filter(F.col(\"json\").isNotNull())\n",
    "    .select(\"key\", \"kafka_ingest_ts\", \"topic\", \"partition\", \"offset\", F.col(\"json.*\"))\n",
    "    .withColumn(\"event_time\", F.to_timestamp(\"event_ts\"))   # assumes ISO-8601\n",
    "    .drop(\"event_ts\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e542f2e-b360-43a1-b24f-68b69301181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = (\n",
    "    parsed\n",
    "    .filter(F.col(\"json\").isNull())\n",
    "    .select(\"key\", \"kafka_ingest_ts\", \"topic\", \"partition\", \"offset\", \"value\")\n",
    "    .withColumn(\"reason\", F.lit(\"SCHEMA_PARSE_FAILED\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34cb021-33ff-47bb-9c59-3f6b927e0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped = (\n",
    "    good\n",
    "    .withWatermark(\"event_time\", WATERMARK)\n",
    "    .dropDuplicates([\"event_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b29770-1171-459b-ae04-bdd36557e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = (\n",
    "    deduped\n",
    "    .withColumn(\"dt\", F.to_date(\"event_time\"))\n",
    "    .withColumn(\"hh\", F.date_format(\"event_time\", \"HH\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04d5315-6dc7-4004-8570-a6bca063ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def publish_alert(subject: str, message: dict):\n",
    "    print(\"publish_alert\", json.dumps(message, default=str))\n",
    "    return \n",
    "    sns.publish(\n",
    "        TopicArn=SNS_TOPIC_ARN,\n",
    "        Subject=subject[:100],\n",
    "        Message=json.dumps(message, default=str)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b68d8a-b8a1-4e5b-b0a8-3cc9d0975de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_OUT = 'S3_OUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf2b409-623b-4a74-824e-e022522ac9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def alerting_and_write(batch_df, batch_id: int):\n",
    "    df = (batch_df\n",
    "          .withColumn(\"device_os\", F.col(\"device.os\"))\n",
    "          .withColumn(\"device_app_ver\", F.col(\"device.app_ver\"))\n",
    "          .drop(\"device\"))\n",
    "\n",
    "    (df.write.mode(\"append\")\n",
    "       .partitionBy(\"dt\", \"hh\")\n",
    "       .csv(S3_OUT))\n",
    "    \n",
    "    # 1) Write good data\n",
    "    #(batch_df\n",
    "    # .write.mode(\"append\")\n",
    "    # .partitionBy(\"dt\", \"hh\")\n",
    "    # .parquet(S3_OUT))\n",
    "\n",
    "    # 2) Compute simple quality metrics per micro-batch (example)\n",
    "    total = batch_df.count()\n",
    "    if total == 0:\n",
    "        publish_alert(\n",
    "            \"Streaming pipeline: zero events\",\n",
    "            {\"batch_id\": batch_id, \"topic\": TOPIC, \"note\": \"No events in this microbatch\"}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "361f699f-15c2-4659-a665-1145fd2a833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'CHECKPOINT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc07870f-0f2f-4de7-b5d2-3fa89f8010cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 775f9ec1-0be3-4b88-a963-365d4d2bee92, runId = 72bab06d-102c-4081-bc0a-fb8346d841c8] terminated with exception: Cannot invoke \"scala.collection.IterableOps.map(scala.Function1)\" because the return value of \"scala.Option.get()\" is null SQLSTATE: XXKST\n=== Streaming Query ===\nIdentifier: [id = 775f9ec1-0be3-4b88-a963-365d4d2bee92, runId = 72bab06d-102c-4081-bc0a-fb8346d841c8]\nCurrent Committed Offsets: {KafkaV2[Subscribe[clickstream_events]]: {\"clickstream_events\":{\"0\":86484}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[clickstream_events]]: {\"clickstream_events\":{\"0\":93483}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\n~WriteToMicroBatchDataSourceV1 ForeachBatchSink, 775f9ec1-0be3-4b88-a963-365d4d2bee92, [checkpointLocation=CHECKPOINT/good], Append\n+- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, event_time#29-T600000ms, dt#31, date_format(event_time#29-T600000ms, HH, Some(Etc/UTC)) AS hh#32]\n   +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, event_time#29-T600000ms, to_date(event_time#29-T600000ms, None, Some(Etc/UTC), true) AS dt#31]\n      +- ~Deduplicate [event_id#18]\n         +- ~EventTimeWatermark f356368c-3e16-401a-9ca3-f30d60304a8d, event_time#29: timestamp, 10 minutes\n            +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, event_time#29]\n               +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, event_ts#20, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, to_timestamp(event_ts#20, None, TimestampType, Some(Etc/UTC), true) AS event_time#29]\n                  +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, json#17.event_id AS event_id#18, json#17.event_type AS event_type#19, json#17.event_ts AS event_ts#20, json#17.user_id AS user_id#21, json#17.session_id AS session_id#22, json#17.order_id AS order_id#23, json#17.product_id AS product_id#24, json#17.amount AS amount#25, json#17.status AS status#26, json#17.device AS device#27, json#17.ip AS ip#28]\n                     +- ~Filter isnotnull(json#17)\n                        +- ~Project [key#14, value#15, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, from_json(StructField(event_id,StringType,false), StructField(event_type,StringType,true), StructField(event_ts,StringType,false), StructField(user_id,StringType,true), StructField(session_id,StringType,true), StructField(order_id,StringType,true), StructField(product_id,StringType,true), StructField(amount,DoubleType,true), StructField(status,StringType,true), StructField(device,StructType(StructField(os,StringType,true),StructField(app_ver,StringType,true)),true), StructField(ip,StringType,true), value#15, Some(Etc/UTC), false) AS json#17]\n                           +- ~Project [cast(key#7 as string) AS key#14, cast(value#8 as string) AS value#15, timestamp#12 AS kafka_ingest_ts#16, topic#9, partition#10, offset#11L]\n                              +- ~StreamingDataSourceV2ScanRelation[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] KafkaTable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamingQueryException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Write streams ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Good stream\u001b[39;00m\n\u001b[32m      3\u001b[39m q1 = (\n\u001b[32m      4\u001b[39m     out.writeStream\n\u001b[32m      5\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     .start()\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mq1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m180\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/streaming/query.py:223\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout <= \u001b[32m0\u001b[39m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    220\u001b[39m             errorClass=\u001b[33m\"\u001b[39m\u001b[33mVALUE_NOT_POSITIVE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m             messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_value\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(timeout).\u001b[34m__name__\u001b[39m},\n\u001b[32m    222\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mStreamingQueryException\u001b[39m: [STREAM_FAILED] Query [id = 775f9ec1-0be3-4b88-a963-365d4d2bee92, runId = 72bab06d-102c-4081-bc0a-fb8346d841c8] terminated with exception: Cannot invoke \"scala.collection.IterableOps.map(scala.Function1)\" because the return value of \"scala.Option.get()\" is null SQLSTATE: XXKST\n=== Streaming Query ===\nIdentifier: [id = 775f9ec1-0be3-4b88-a963-365d4d2bee92, runId = 72bab06d-102c-4081-bc0a-fb8346d841c8]\nCurrent Committed Offsets: {KafkaV2[Subscribe[clickstream_events]]: {\"clickstream_events\":{\"0\":86484}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[clickstream_events]]: {\"clickstream_events\":{\"0\":93483}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\n~WriteToMicroBatchDataSourceV1 ForeachBatchSink, 775f9ec1-0be3-4b88-a963-365d4d2bee92, [checkpointLocation=CHECKPOINT/good], Append\n+- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, event_time#29-T600000ms, dt#31, date_format(event_time#29-T600000ms, HH, Some(Etc/UTC)) AS hh#32]\n   +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, event_time#29-T600000ms, to_date(event_time#29-T600000ms, None, Some(Etc/UTC), true) AS dt#31]\n      +- ~Deduplicate [event_id#18]\n         +- ~EventTimeWatermark f356368c-3e16-401a-9ca3-f30d60304a8d, event_time#29: timestamp, 10 minutes\n            +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, event_time#29]\n               +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, event_id#18, event_type#19, event_ts#20, user_id#21, session_id#22, order_id#23, product_id#24, amount#25, status#26, device#27, ip#28, to_timestamp(event_ts#20, None, TimestampType, Some(Etc/UTC), true) AS event_time#29]\n                  +- ~Project [key#14, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, json#17.event_id AS event_id#18, json#17.event_type AS event_type#19, json#17.event_ts AS event_ts#20, json#17.user_id AS user_id#21, json#17.session_id AS session_id#22, json#17.order_id AS order_id#23, json#17.product_id AS product_id#24, json#17.amount AS amount#25, json#17.status AS status#26, json#17.device AS device#27, json#17.ip AS ip#28]\n                     +- ~Filter isnotnull(json#17)\n                        +- ~Project [key#14, value#15, kafka_ingest_ts#16, topic#9, partition#10, offset#11L, from_json(StructField(event_id,StringType,false), StructField(event_type,StringType,true), StructField(event_ts,StringType,false), StructField(user_id,StringType,true), StructField(session_id,StringType,true), StructField(order_id,StringType,true), StructField(product_id,StringType,true), StructField(amount,DoubleType,true), StructField(status,StringType,true), StructField(device,StructType(StructField(os,StringType,true),StructField(app_ver,StringType,true)),true), StructField(ip,StringType,true), value#15, Some(Etc/UTC), false) AS json#17]\n                           +- ~Project [cast(key#7 as string) AS key#14, cast(value#8 as string) AS value#15, timestamp#12 AS kafka_ingest_ts#16, topic#9, partition#10, offset#11L]\n                              +- ~StreamingDataSourceV2ScanRelation[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] KafkaTable\n"
     ]
    }
   ],
   "source": [
    "# --- Write streams ---\n",
    "# Good stream\n",
    "q1 = (\n",
    "    out.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(alerting_and_write)\n",
    "    .option(\"checkpointLocation\", CHECKPOINT + \"/good\")\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .start()\n",
    ")\n",
    "q1.awaitTermination(180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc7e572-fb40-48b6-b59d-73580ea5501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BAD = 'S3_BAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c926012-3099-44c5-b6fc-2d69173c6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad stream to S3 (DLQ)\n",
    "q2 = (\n",
    "    bad.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"csv\") #(\"parquet\")\n",
    "    .option(\"path\", S3_BAD)\n",
    "    .option(\"checkpointLocation\", CHECKPOINT + \"/bad\")\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b963f-bc24-4b7c-9045-9d89b8027092",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"q1 active:\", q1.isActive, \"id:\", q1.id)\n",
    "print(\"q2 active:\", q2.isActive, \"id:\", q2.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb81dc-e382-448e-8943-5a40f6c44706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350c23b-a845-4947-82f4-9cd91abd6ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
